<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP | AndSt</title>
    <link>https://andst.github.io/tag/nlp/</link>
      <atom:link href="https://andst.github.io/tag/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>NLP</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 04 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://andst.github.io/media/icon_huf55df3efd36cb4bdb6539523da3e6974_26772_512x512_fill_lanczos_center_3.png</url>
      <title>NLP</title>
      <link>https://andst.github.io/tag/nlp/</link>
    </image>
    
    <item>
      <title>My dive into Causal Inference and thoughts on the connection to NLP</title>
      <link>https://andst.github.io/post/causality_nlp_opinion/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://andst.github.io/post/causality_nlp_opinion/</guid>
      <description>&lt;p&gt;After starting my PhD I aimed to broaden my interests in machine learning.&lt;/p&gt;
&lt;h2 id=&#34;what-is-causal-inference&#34;&gt;What is Causal Inference?&lt;/h2&gt;
&lt;p&gt;Within this paragraph I will give a high level view which lacks completeness and any formal correctness . The goal is to give you an intuitive idea how I see the field. Given my machine learning background, my limited exposure to the field, chances are that my introduction is not the best for everyone.
If you leave this intro with a feeling of some questions Causal Inference aims to answer, I&amp;rsquo;d be really happy ü•≥.
Now let&amp;rsquo;s get down to business.&lt;/p&gt;
&lt;p&gt;Causal thinking is something very natural to humans. A child easily understands the causal relationship between falling down and the pain it creates. Falling is the cause and pain is the effect.
One goal of Causal Inference is to mathematically formalize this thinking.&lt;/p&gt;
&lt;p&gt;There are many examples which show that the traditional statistical language, which is based on the notion of independence, is incapable to describe these relationships. A famous saying hereto is association is not causation which is described by Simpson&amp;rsquo;s Paradox. A traditional example is the UC Berkeley gender bias case. In total, the numbers showed that UCB was biased to admit more men. But a closer inspection showed that woman often applied to more competitive degrees where more people were rejected. Thus in total the trend was not that clear. The example shows that a statistical fact doesn&amp;rsquo;t describe the cause and effect of the underlying process. Here, in the language of Causal Inference - more later -, a so called confounding variable (the degree program) conflicted with standard statistics.&lt;/p&gt;
&lt;p&gt;Now I want to give a tiny intro into the mathematical modelling. Causal Inference assumes operates on so called Strucural Causal Models (SCM), often also called Structural Equation Model (SEM), which generates a causal graph.&lt;/p&gt;
&lt;p&gt;A SCM $C := (S, N)$ governing the random vector $X = (X1, . . . , Xd)$ is a set of structural equations: &lt;br&gt;
$S_i: X_i \leftarrow fi(Pa(X_i), N_i)$, &lt;br&gt;
where $Pa(Xi) ‚äÜ {X_1, . . . , X_d} \ {X_i}$ are called the parents of Xi, and the $N_i$ are
independent noise random variables. We say that ‚Äú$X_i$ causes $X_j$‚Äù if $X_i ‚àà Pa(X_j)$. &lt;br&gt;
We call a graph $G$ a causal graph of $X$ if it is obtained by drawing i) one node for each $X_i$,
and ii) one edge from $X_i$ to $X_j$ if $X_i ‚àà Pa(X_j)$. We assume acyclic causal graphs. &lt;br&gt;
Definition is taken from [1]. Various abstractions, including cyclic graphs are also possible, e.g. [2].&lt;/p&gt;
&lt;p&gt;One task is to observe SCM&amp;rsquo;s or causal graphs from observational data.
Most of the time we assume they are given by human domain knowledge. To answer causal questions humans resort to &amp;ldquo;What if&amp;rdquo; questions, e.g. &amp;ldquo;Would it have been smarter if I had used the bike instead of the car&amp;rdquo;. This is formalized by interventions. Again we take the definition from [1]:&lt;/p&gt;
&lt;p&gt;Consider a SEM C = (S, N). An intervention $e$ on $C$ consists of replacing one or several of its structural equations to obtain an intervened SEM $C^e=(S^e, N^e)$, with structural equations: &lt;br&gt;
$S_i^e : X_i \leftarrow f_i^e (Pa^e(X_i^e), N_i^e)$,.&lt;br&gt;
The variable $X_e$ is intervened if $S_i \neq S_i^e$ or $N_i \neq N_i^e$.&lt;/p&gt;
&lt;p&gt;A hard intervention would set a value $X_i$ to a specific value, e.g. man or woman. A soft intervention would change the formula. By doing so we can answer &amp;ldquo;What if&amp;rdquo; questions.&lt;/p&gt;
&lt;p&gt;In the end, I want to talk about Pearl&amp;rsquo;s ladder of Causal inference. I hope this gives some high level insight into the types of questions Causal Inference aims to answer.&lt;/p&gt;
&lt;h2 id=&#34;how-did-i-learn-it&#34;&gt;How did I learn it?&lt;/h2&gt;
&lt;p&gt;While I heard about the field and the famous saying &amp;ldquo;Causation is not equal to Correlation&amp;rdquo;, I just started diving into the topic this January. With a good friend we went through the course by Brady Neal (&lt;a href=&#34;https://www.bradyneal.com/causal-inference-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to course&lt;/a&gt;). I really enjoyed learning the theory here. It establishes the philosophical thoughts and basic assumptions on the basis of easy examples. It&amp;rsquo;s important to note that the course doesn&amp;rsquo;t focus on the relation to machine learning but gives a general introduction to the topic. In my view this is necessary to get a comprehensive introduction.&lt;/p&gt;
&lt;p&gt;TODO: compare to primer and sch√∂lkopf book.&lt;/p&gt;
&lt;h2 id=&#34;what-connections-did-i-observe-to-nlp&#34;&gt;What connections did I observe to NLP?&lt;/h2&gt;
&lt;p&gt;On the first sight, causal thinking is completely natural to human thinking. We make statistical and causal considerations in most of our arguments.
#TODO good example
But when you want to apply the theory to certain problems, often complexity rises to quickly. Most problems in causal inference assume a known causal graph. In language, every conversation can be represented by a different graph. This is an example of the complexity arising.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s different ways how Causal Inference interacts with machine learning in general. A good overview is given in  TODO: Sch√∂lkopf paper&lt;/p&gt;
&lt;p&gt;githu causaltext link
An idea is to use CI for model explainability. A very nice example is CausaLM&lt;/p&gt;
&lt;h2 id=&#34;what-problems-do-i-see&#34;&gt;What problems do I see?&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
